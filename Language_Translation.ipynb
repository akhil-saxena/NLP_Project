{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required modules (numpy, re, tensorflow, keras[layers, models])\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the file from dataset Tatoeba Project\n",
    "filename=\"C:\\\\Users\\\\saxen\\\\Desktop\\\\Dataset\\cmn.txt\"\n",
    "with open(filename,'r', encoding='utf-8') as file: # 'with open' closes the file when it is no longer required\n",
    "     lines = file.read().split('\\n') # read each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data in a lists\n",
    "docs_in = []\n",
    "docs_out= []\n",
    "\n",
    "# Vocabulary sets\n",
    "tokens_in = set()\n",
    "tokens_out = set()\n",
    "\n",
    "# Change the value '150' to read more lines from file\n",
    "#(The program may take longer to run in that case.)\n",
    "for line in lines[:100]:\n",
    "    doc_in, doc_out = line.split('\\t')[:2]\n",
    "    # Append input sentences to docs_in\n",
    "    docs_in.append(doc_in)\n",
    "    doc_out=\" \".join(re.findall(r\"[\\w]+[^\\s\\w]\", doc_out))\n",
    "    # add <START/END> tags to each output sentence\n",
    "    doc_out='<START> '+ doc_out + ' END'\n",
    "    # Append output sentences to docs_out\n",
    "    docs_out.append(doc_out)\n",
    "    \n",
    "    # Split input sentences to words from each sentence\n",
    "    # Add each unique word only once\n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", doc_in):\n",
    "        if token not in tokens_in:\n",
    "            tokens_in.add(token)\n",
    "    # Split output sentences to words from each sentence\n",
    "    # Add each unique word only once\n",
    "    for token in doc_out.split():\n",
    "        if token not in tokens_out:\n",
    "            tokens_out.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the set of tokens\n",
    "tokens_in = sorted(list(tokens_in))\n",
    "tokens_out = sorted(list(tokens_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder & Decoder\n",
    "num_encoder_tokens = len(tokens_in)\n",
    "num_decoder_tokens = len(tokens_out)\n",
    "\n",
    "# Assign seq length as max length from each word in each line in list of sentences.\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", doc_in)) for doc_out in docs_in])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", doc_out)) for doc_out in docs_out])\n",
    "\n",
    "# Token index as a dict for reference\n",
    "input_features_dict = dict([(token, i) for i, token in enumerate(tokens_in)])\n",
    "target_features_dict = dict([(token, i) for i, token in enumerate(tokens_out)])\n",
    "\n",
    "# Reverse token index as a dict for reference\n",
    "reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "# Create arrays for max possible length with all values initialised as zeros.\n",
    "encoder_input_data = np.zeros((len(docs_in), max_encoder_seq_length, num_encoder_tokens), dtype= 'float32')\n",
    "decoder_input_data = np.zeros((len(docs_in), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(docs_in), max_decoder_seq_length, num_decoder_tokens), dtype= 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over bot input and output simultaneously using enumerate(zip(l1,l2))\n",
    "for line, (doc_in, doc_out) in enumerate(zip(docs_in, docs_out)):\n",
    "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", doc_in)):\n",
    "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "    for timestep, token in enumerate(doc_out.split()):\n",
    "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "        if timestep > 0:\n",
    "            decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=256 # Dimensionality\n",
    "batch_size=100 # No. of samples processed in one iteration\n",
    "epochs=100 # No. of passes over the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder training using LSTM\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder training using LSTM, Dense\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 105)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 370688      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 93)     23901       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 752,989\n",
      "Trainable params: 752,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a training model \n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(\"Model summary:\")\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model:\n",
      "\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 1.1399 - accuracy: 0.0016 - val_loss: 1.1168 - val_accuracy: 0.1250\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.1212 - accuracy: 0.1250 - val_loss: 1.1018 - val_accuracy: 0.1250\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1023 - accuracy: 0.1250 - val_loss: 1.0796 - val_accuracy: 0.1250\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0748 - accuracy: 0.1250 - val_loss: 1.0434 - val_accuracy: 0.1250\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0293 - accuracy: 0.1250 - val_loss: 0.9811 - val_accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9488 - accuracy: 0.1250 - val_loss: 0.8738 - val_accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8093 - accuracy: 0.1250 - val_loss: 0.7205 - val_accuracy: 0.1250\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6413 - accuracy: 0.1250 - val_loss: 0.6421 - val_accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6021 - accuracy: 0.1250 - val_loss: 0.6539 - val_accuracy: 0.1250\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.1250 - val_loss: 0.6562 - val_accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.1250 - val_loss: 0.6595 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5885 - accuracy: 0.1250 - val_loss: 0.6626 - val_accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.1250 - val_loss: 0.6658 - val_accuracy: 0.1250\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.1250 - val_loss: 0.6690 - val_accuracy: 0.1250\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.1250 - val_loss: 0.6722 - val_accuracy: 0.1250\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5765 - accuracy: 0.1250 - val_loss: 0.6754 - val_accuracy: 0.1250\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5739 - accuracy: 0.1250 - val_loss: 0.6787 - val_accuracy: 0.1250\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.1250 - val_loss: 0.6821 - val_accuracy: 0.1250\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.1250 - val_loss: 0.6855 - val_accuracy: 0.1250\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.1250 - val_loss: 0.6891 - val_accuracy: 0.1250\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.1250 - val_loss: 0.6927 - val_accuracy: 0.1250\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.1250 - val_loss: 0.6965 - val_accuracy: 0.1250\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.1250 - val_loss: 0.7003 - val_accuracy: 0.1250\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.1250 - val_loss: 0.7043 - val_accuracy: 0.1250\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.1250 - val_loss: 0.7085 - val_accuracy: 0.1250\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.1250 - val_loss: 0.7128 - val_accuracy: 0.1250\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.1250 - val_loss: 0.7173 - val_accuracy: 0.1250\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5468 - accuracy: 0.1250 - val_loss: 0.7219 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.1250 - val_loss: 0.7268 - val_accuracy: 0.1250\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.1250 - val_loss: 0.7318 - val_accuracy: 0.1250\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.1250 - val_loss: 0.7371 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.1250 - val_loss: 0.7426 - val_accuracy: 0.1250\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.1250 - val_loss: 0.7483 - val_accuracy: 0.1250\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.1250 - val_loss: 0.7542 - val_accuracy: 0.1250\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.1250 - val_loss: 0.7604 - val_accuracy: 0.1250\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.1250 - val_loss: 0.7668 - val_accuracy: 0.1250\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.1250 - val_loss: 0.7734 - val_accuracy: 0.1250\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.1250 - val_loss: 0.7801 - val_accuracy: 0.1250\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.1250 - val_loss: 0.7870 - val_accuracy: 0.1250\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.1250 - val_loss: 0.7940 - val_accuracy: 0.1250\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.1250 - val_loss: 0.8011 - val_accuracy: 0.1250\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4972 - accuracy: 0.1250 - val_loss: 0.8083 - val_accuracy: 0.1250\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4921 - accuracy: 0.1281 - val_loss: 0.8157 - val_accuracy: 0.1250\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.1297 - val_loss: 0.8232 - val_accuracy: 0.1250\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.1328 - val_loss: 0.8309 - val_accuracy: 0.1250\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.1344 - val_loss: 0.8390 - val_accuracy: 0.1250\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.1375 - val_loss: 0.8476 - val_accuracy: 0.1250\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.1375 - val_loss: 0.8569 - val_accuracy: 0.1250\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4519 - accuracy: 0.1375 - val_loss: 0.8669 - val_accuracy: 0.1250\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.1391 - val_loss: 0.8790 - val_accuracy: 0.1250\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.1391 - val_loss: 0.8900 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.1406 - val_loss: 0.9210 - val_accuracy: 0.1250\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.1375 - val_loss: 0.9359 - val_accuracy: 0.1312\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.1562 - val_loss: 0.9889 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.1266 - val_loss: 0.9240 - val_accuracy: 0.1250\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.1594 - val_loss: 0.9365 - val_accuracy: 0.1250\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.1594 - val_loss: 0.9447 - val_accuracy: 0.1250\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.1641 - val_loss: 0.9545 - val_accuracy: 0.1250\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.1719 - val_loss: 0.9646 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3587 - accuracy: 0.1828 - val_loss: 0.9756 - val_accuracy: 0.1250\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.1922 - val_loss: 0.9869 - val_accuracy: 0.1250\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.2000 - val_loss: 0.9985 - val_accuracy: 0.1250\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.2062 - val_loss: 1.0099 - val_accuracy: 0.1250\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.2109 - val_loss: 1.0210 - val_accuracy: 0.1250\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.2141 - val_loss: 1.0319 - val_accuracy: 0.1250\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.2219 - val_loss: 1.0405 - val_accuracy: 0.1250\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.2234 - val_loss: 1.0562 - val_accuracy: 0.1312\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.2250 - val_loss: 1.0496 - val_accuracy: 0.1312\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.2281 - val_loss: 1.1556 - val_accuracy: 0.1250\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.1500 - val_loss: 1.0771 - val_accuracy: 0.1250\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.2156 - val_loss: 1.1336 - val_accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.1828 - val_loss: 1.0778 - val_accuracy: 0.1312\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.2297 - val_loss: 1.0739 - val_accuracy: 0.1312\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.2297 - val_loss: 1.0762 - val_accuracy: 0.1312\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.2297 - val_loss: 1.0809 - val_accuracy: 0.1312\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.2297 - val_loss: 1.0863 - val_accuracy: 0.1312\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.2297 - val_loss: 1.0916 - val_accuracy: 0.1312\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.2313 - val_loss: 1.0966 - val_accuracy: 0.1312\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.2328 - val_loss: 1.1012 - val_accuracy: 0.1312\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.2344 - val_loss: 1.1055 - val_accuracy: 0.1312\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.2344 - val_loss: 1.1095 - val_accuracy: 0.1312\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.2359 - val_loss: 1.1131 - val_accuracy: 0.1312\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.2359 - val_loss: 1.1166 - val_accuracy: 0.1312\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.2359 - val_loss: 1.1198 - val_accuracy: 0.1312\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.2359 - val_loss: 1.1229 - val_accuracy: 0.1312\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.2359 - val_loss: 1.1260 - val_accuracy: 0.1312\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.2359 - val_loss: 1.1290 - val_accuracy: 0.1312\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.2359 - val_loss: 1.1321 - val_accuracy: 0.1312\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.2359 - val_loss: 1.1352 - val_accuracy: 0.1312\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.2359 - val_loss: 1.1387 - val_accuracy: 0.1312\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.2359 - val_loss: 1.1414 - val_accuracy: 0.1312\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.2359 - val_loss: 1.1469 - val_accuracy: 0.1312\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.2359 - val_loss: 1.1429 - val_accuracy: 0.1312\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.2359 - val_loss: 1.1817 - val_accuracy: 0.1312\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2359 - val_loss: 1.1194 - val_accuracy: 0.1250\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.2109 - val_loss: 1.3842 - val_accuracy: 0.1250\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.1250 - val_loss: 1.1609 - val_accuracy: 0.1250\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.2359 - val_loss: 1.1612 - val_accuracy: 0.1250\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.2359 - val_loss: 1.1621 - val_accuracy: 0.1250\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.2359 - val_loss: 1.1634 - val_accuracy: 0.1312\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Training the model:\\n\")\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "training_model.save('lang_translate_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(test_input):\n",
    "    states_value = encoder_model.predict(test_input)\n",
    "    # Create empty output_seq of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Initialise the first value in each as '<START>' label\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    # Run the model to get possible outputs with probabiliy\n",
    "    while not stop_condition:\n",
    "        output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "        # Choose the output with max probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        # Exit if '<END>' or max length reached\n",
    "        if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # Update the output_seq\n",
    "        #target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [hidden_state, cell_state]\n",
    "    # return the sentence selected    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Input sentence: Listen.\n",
      "Decoded sentence:  听着。 END END\n",
      "-----\n",
      "Input sentence: No way!\n",
      "Decoded sentence:  没门！ END END\n",
      "-----\n",
      "Input sentence: No way!\n",
      "Decoded sentence:  没门！ END END\n",
      "-----\n",
      "Input sentence: Really?\n",
      "Decoded sentence:  你确定？ END\n",
      "-----\n",
      "Input sentence: Try it.\n",
      "Decoded sentence:  试试吧。 END\n",
      "-----\n",
      "Input sentence: We try.\n",
      "Decoded sentence:  我们来试试。 END\n",
      "-----\n",
      "Input sentence: Why me?\n",
      "Decoded sentence:  为什么是我？ END\n",
      "-----\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence:  去问汤姆。 END\n",
      "-----\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence:  好棒！ END END\n",
      "-----\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence:  冷静点。 END\n"
     ]
    }
   ],
   "source": [
    "for i in range(15, 25):\n",
    "    temp_in = encoder_input_data[i: i + 1]\n",
    "    decoded_sentence = decode_sequence(temp_in)\n",
    "    print('-----')\n",
    "    print('Input sentence:', docs_in[i])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
